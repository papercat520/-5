{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# 自注意力层\n",
    "class Self_Attention(Layer):\n",
    "    # input:  [None, n, k]输入为n个维度为k的词向量\n",
    "    # mask:   [None, n]表示填充词位置的mask\n",
    "    # output: [None, k]输出n个词向量的加权和\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.k = input_shape[0][-1]  #词向量维度\n",
    "        self.W_layer = Dense(self.k, activation='tanh', use_bias=True) #对h的映射\n",
    "        self.U_weight = self.add_weight(name='U', shape=(self.k, 1),   #U记忆矩阵\n",
    "                                        initializer=tf.keras.initializers.glorot_uniform(),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input, mask = inputs #输入有两部分[input, mask]\n",
    "        if K.ndim(input) != 3:\n",
    "            raise ValueError(\"The dim of inputs is required 3 but get {}\".format(K.ndim(input)))\n",
    "\n",
    "        # 计算score\n",
    "        x = self.W_layer(input)              # [None, n, k]\n",
    "        score = tf.matmul(x, self.U_weight)  # [None, n, 1]\n",
    "        score = self.dropout_layer(score)    # 随机dropout(也可不要)\n",
    "\n",
    "        # softmax之前进行mask\n",
    "        mask = tf.expand_dims(mask, axis=-1)  # [None, n, 1]\n",
    "        padding = tf.cast(tf.ones_like(mask)*(-2**31+1), tf.float32) #mask的位置填充很小的负数\n",
    "        score = tf.where(tf.equal(mask, 0), padding, score)\n",
    "        score = tf.nn.softmax(score, axis=1)  # [None, n, 1] mask之后计算softmax\n",
    "\n",
    "        # 向量加权和\n",
    "        output = tf.matmul(input, score, transpose_a=True)   \n",
    "        output /= self.k**0.5                                \n",
    "        output = tf.squeeze(output, axis=-1)                 \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Text_Attention(Layer):\n",
    "    # 该层的输入有三部分image_emb、seq_emb、mask\n",
    "    # image_emb: [None, M, 4096]对应M个4096维的图像向量(由vgg16提取得到)，每条评论的M可以不一致\n",
    "    # seq_emb:   [None, L, k]表示L个维度为k的句向量\n",
    "    # mask:      [None, L]表示L个句子的mask(因为存在句子数不足L的文档，有被padding的句子)\n",
    "    # output:    [None, M, k]输出为M个图像对应的文档向量表示\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(Image_Text_Attention, self).__init__()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.l = input_shape[1][1]   # 句子个数\n",
    "        self.k = input_shape[1][-1]  # 句向量维度\n",
    "        self.img_layer = Dense(1, activation='tanh', use_bias=True)  # 将image_emb映射到1维\n",
    "        self.seq_layer = Dense(1, activation='tanh', use_bias=True)  # 将seq_emb也映射到1维(方便内积)\n",
    "        self.V_weight = self.add_weight(name='V', shape=(self.l, self.l),\n",
    "                                        initializer=tf.keras.initializers.glorot_uniform(),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        image_emb, seq_emb, mask = inputs  # 输入为三部分[image_emb, seq_emb, mask]\n",
    "\n",
    "        # 线性映射\n",
    "        p = self.img_layer(image_emb)  # [None, M, 1]\n",
    "        q = self.seq_layer(seq_emb)    # [None, L, 1]\n",
    "\n",
    "        # 内积+映射(计算score)\n",
    "        emb = tf.matmul(p, q, transpose_b=True)   # [None, M, L]\n",
    "        emb = emb + tf.transpose(q, [0, 2, 1])    # [None, M, L]\n",
    "        emb = tf.matmul(emb, self.V_weight)       # [None, M, L]\n",
    "        score = self.dropout_layer(emb)           # 随机dropout(也可不要)\n",
    "\n",
    "        # mask\n",
    "        mask = tf.tile(tf.expand_dims(mask, axis=1), [1, score.shape[1], 1])  # [None, M, L]，将mask矩阵复制到与score相同的形状\n",
    "        padding = tf.cast(tf.ones_like(mask) * (-2 ** 31 + 1), tf.float32)\n",
    "        score = tf.where(tf.equal(mask, 0), padding, score)\n",
    "        score = tf.nn.softmax(score, axis=-1)      # [None, M, L]\n",
    "\n",
    "        # 向量加权和\n",
    "        output = tf.matmul(score, seq_emb)   # [None, M, k]\n",
    "        output /= self.k**0.5                # 归一化\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Vgg16Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # （输入通道，输出通道，卷积核大小） 例：32*32*3 —> (32+2*1-3)/1+1 = 32，输出：32*32*64\n",
    "            nn.Conv2d(3, 64, 3, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # （输入通道，输出通道，卷积核大小） 输入：32*32*64，卷积：3*64*64，输出：32*32*64\n",
    "            nn.Conv2d(64, 64, 3, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   # 输入：32*32*64，输出：6*16*64\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            self.layer1,\n",
    "            self.layer2,\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "            self.layer5,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(1000, 82),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Bidirectional\n",
    "\n",
    "class VistaNet(Model):\n",
    "    def __init__(self, block_nums=[2,2,3,3,3], out_dim=4096, vgg_dropout=0.0, attention_dropout=0.0, gru_units=[64, 128], class_num=5):\n",
    "        # block_nums: vgg16各层卷积的个数\n",
    "        # out_dim: vgg16输出维度\n",
    "        # dropout: 各层的dropout系数\n",
    "        # gru_units: 两个单层双向GRU的输出维度\n",
    "        # class_num： 模型最终输出维度\n",
    "        super(VistaNet, self).__init__()\n",
    "        self.vgg16 = Vgg16Net(block_nums, out_dim, vgg_dropout)       # VGG-16\n",
    "        self.word_self_attention = Self_Attention(attention_dropout)# 第一层中的自注意力\n",
    "        self.img_seq_attention = Image_Text_Attention(attention_dropout)  # 第二层中的Image-Text注意力\n",
    "        self.doc_self_attention = Self_Attention(attention_dropout) # 第三层中的自注意力\n",
    "        # 两个单层双向GRU层\n",
    "        self.BiGRU_layer1 = Bidirectional(GRU(units=gru_units[0],\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                             recurrent_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                             return_sequences=True),\n",
    "                                          merge_mode='concat')\n",
    "        self.BiGRU_layer2 = Bidirectional(GRU(units=gru_units[1],\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                             recurrent_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                             return_sequences=True),\n",
    "                                          merge_mode='concat')\n",
    "        self.output_layer = Dense(class_num, activation='softmax') # 任务层\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # 输入inputs包含三部分：(假设batchsize为1，省略掉第一维None)\n",
    "        # image_inputs: [M, 227, 227, 3]一条评论样本包含的M个图像\n",
    "        # text_inputs:  [L, T, k]一条样本表示一个文档，所以输入张量为3维:[最大句子数，最大单词数， 词向量维度]\n",
    "        # mask: [L, T]每句话中mask词的位置\n",
    "        image_inputs, text_inputs, mask = inputs \n",
    "\n",
    "        # 获取图像emb向量\n",
    "        image_emb = self.vgg16(image_inputs)       # [M, 224, 224, 3] -> [M, 4096]\n",
    "\n",
    "        # 经过GRU层获取词向量word_emb\n",
    "        word_emb = self.BiGRU_layer1(text_inputs)  # [L, T, k] -> [L, T, 2k]\n",
    "\n",
    "        # 经过self_attention得到句向量seq_emb\n",
    "        input = [word_emb, mask]                   # [L, T, 2k] & [L, T]\n",
    "        seq_emb = self.word_self_attention(input)  # [L, T, 2k] -> [L, 2k]\n",
    "\n",
    "        # 经过GRU层提取语义\n",
    "        input = tf.expand_dims(seq_emb, axis=0)    # [1, L, 2k]\n",
    "        seq_emb = self.BiGRU_layer2(input)         # [1, L, 2k] -> [1, L, 4k]\n",
    "\n",
    "        # 经过img_seq_attention得到M个文档向量doc_emb\n",
    "        image_emb = tf.expand_dims(image_emb, axis=0) # [1, M, 4096]\n",
    "        mask = tf.argmax(mask, axis=1)                # [L, ]\n",
    "        mask = tf.expand_dims(mask, axis=0)           # [1, L]\n",
    "        input = [image_emb, seq_emb, mask]\n",
    "        doc_emb = self.img_seq_attention(input)       # [1, M, 4k] M个文档向量表示\n",
    "\n",
    "        # 经过self_attention得到最终的文档向量\n",
    "        mask = tf.ones(shape=[1, doc_emb.shape[1]])   # [1, M],全为非0值，因为该注意力无需mask\n",
    "        input = [doc_emb, mask]\n",
    "        D_emb = self.doc_self_attention(input)        # [1, 4k]\n",
    "\n",
    "        # output layer\n",
    "        output = self.output_layer(D_emb)             # [1, class_num]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"vgg_net_2\" (type VggNet).\n\n'JpegImageFile' object has no attribute 'shape'\n\nCall arguments received:\n  • inputs=<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=551x325 at 0x159147A3850>\n  • kwargs={'training': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cf1fc821b6f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-1871c7d12d92>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# 获取图像emb向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mimage_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_inputs\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# [M, 224, 224, 3] -> [M, 4096]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# 经过GRU层获取词向量word_emb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b1bedcb03ef1>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# 标准输入：[batchsize, 224, 224, 3]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The dim of inputs is required 4 but get {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Exception encountered when calling layer \"vgg_net_2\" (type VggNet).\n\n'JpegImageFile' object has no attribute 'shape'\n\nCall arguments received:\n  • inputs=<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=551x325 at 0x159147A3850>\n  • kwargs={'training': 'None'}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "model = VistaNet()\n",
    "\n",
    "for i in range(1,5129):\n",
    "    img = Image.open(\"C:/Users/15328/Desktop/information/大三下/当代人工智能/exp5data/实验五数据/实验五数据/data/\"+str(i)+\".jpg\")\n",
    "    path = r'C:/Users/15328/Desktop/information/大三下/当代人工智能/exp5data/实验五数据/实验五数据/data/'+str(i)+'.txt'\n",
    "    file1 = open(path,'r')\n",
    "    text_input = file1.read()\n",
    "    mask = (i,i)\n",
    "    input = [img, text_input, mask]\n",
    "    pre = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
